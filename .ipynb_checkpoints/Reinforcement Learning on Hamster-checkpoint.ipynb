{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning on Hamster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym' has no attribute 'make'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aa66710cc56e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HamsterExperiment-v0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'gym' has no attribute 'make'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('HamsterExperiment-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# introduce map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()   \n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        \n",
    "        # Set epsilon\n",
    "        epsilon = 1./(1.+i_episode)\n",
    "        # Observe S_0, t<-0\n",
    "        state = env.reset()\n",
    "        # Get A_0 from Q (epsilon-greedy policy) for this state\n",
    "        policy_state = epsilon_greedy_policy(env, Q[state], epsilon)\n",
    "        action = np.random.choice(np.arange(env.nA), p=policy_state)\n",
    "        \n",
    "        # Repeat until terminal state reached\n",
    "        while True:\n",
    "            # Take A_t, get R_(t+1), S_(t+1)\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Choose A_(t+1) from Q (from policy for S_(t+1))\n",
    "            policy_state = epsilon_greedy_policy(env, Q[state_next], epsilon)\n",
    "            action_next = np.random.choice(np.arange(env.nA), p=policy_state)\n",
    "            # Get G_t\n",
    "            G_t = reward + gamma*Q[state_next][action_next]\n",
    "            # Update action value function\n",
    "            Q[state][action] = Q[state][action] + alpha*(G_t - Q[state][action])\n",
    "            print (Q[state][action])\n",
    "            \n",
    "            # print\n",
    "            env.render()\n",
    "            \n",
    "            \n",
    "            # Check if reached terminal state\n",
    "            if done:\n",
    "                break\n",
    "            # Update state & action for next step\n",
    "            state = state_next\n",
    "            action = action_next\n",
    "        \n",
    "    return Q\n",
    "\n",
    "def epsilon_greedy_policy(env, Q_state, epsilon):\n",
    "    # Get greedy action (gives highest Q for state)\n",
    "    greedy_action = np.argmax(Q_state)\n",
    "    # Get number of possible actions     \n",
    "    nA = env.nA\n",
    "    # Use epsilon to get probability distribution to use in policy for state\n",
    "    policy_state = np.ones(nA) * epsilon / nA\n",
    "    policy_state[greedy_action] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsa = sarsa(env, 5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
